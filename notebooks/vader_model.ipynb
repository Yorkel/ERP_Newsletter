{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb902065-390d-4341-8645-a2492ba7db16",
   "metadata": {},
   "source": [
    "# Imports & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814ee64c-aaee-4918-bc85-8701f82f15a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e1fafa-b4ef-48cc-b364-909788e3a5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 300 rows\n",
      "Loaded 300 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['id', 'newsletter_number', 'issue_date', 'new_theme', 'text', 'domain',\n",
       "       'organisation', 'org_group', 'year_quarter'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "\n",
    "sample = pd.read_csv(\"/workspaces/ERP_Newsletter/data_processed/sample_300_full.csv\")\n",
    "labels = pd.read_csv(\"/workspaces/ERP_Newsletter/data_processed/sample_llm_prelabeled.csv\")\n",
    "\n",
    "print(f\"Loaded {len(labels)} rows\")\n",
    "print(f\"Loaded {len(sample)} rows\")\n",
    "\n",
    "sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1cdfb50-b604-4fdb-8cfb-44abfde904fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "labels = labels.rename(columns={\"doc_id\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bcad6dc-c24a-46ba-885d-f65db8b7adc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged dataset shape: (300, 12)\n",
      "Columns: ['id', 'newsletter_number', 'issue_date', 'new_theme', 'text', 'domain', 'organisation', 'org_group', 'year_quarter', 'llm_label', 'llm_confidence', 'llm_rationale']\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(sample, labels, on=[\"id\", \"text\"], how=\"inner\")\n",
    "print(f\"✅ Merged dataset shape: {df.shape}\")\n",
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c2c5978-046e-40f4-9404-2119698f1ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Columns retained: ['id', 'text', 'new_theme', 'organisation', 'org_group', 'year_quarter', 'llm_label', 'llm_confidence']\n"
     ]
    }
   ],
   "source": [
    "keep_cols = [\n",
    "    \"id\",\n",
    "    \"text\",\n",
    "    \"new_theme\",\n",
    "    \"organisation\",\n",
    "    \"org_group\",\n",
    "    \"year_quarter\",\n",
    "    \"llm_label\",\n",
    "    \"llm_confidence\"\n",
    "]\n",
    "\n",
    "df = df[keep_cols].copy()\n",
    "\n",
    "print(f\"✅ Columns retained: {keep_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45f8ce-b0d9-4cd2-adca-0af689926f62",
   "metadata": {},
   "source": [
    "# Light Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b401ef5-5d07-4c78-af9f-6127b3689c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_for_vader'] = df['text'].fillna('')  # Ensure no NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "751c9f89-9043-4a43-ba11-9d1bf2eec986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove URLs \n",
    "df['text_for_vader'] = df['text_for_vader'].str.replace(\n",
    "    r'http\\S+|www\\S+', '', regex=True\n",
    ").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "272f8b47-f67f-40ba-b932-d247c3e7a5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    300.000000\n",
      "mean      42.133333\n",
      "std       21.375029\n",
      "min        6.000000\n",
      "25%       26.000000\n",
      "50%       37.000000\n",
      "75%       52.000000\n",
      "max      125.000000\n",
      "Name: text_for_vader, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check text lengths (VADER works better on sentences/paragraphs)\n",
    "print(df['text_for_vader'].str.split().str.len().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f5dd8-a54a-4648-9445-77c7e95bcdbe",
   "metadata": {},
   "source": [
    "# Train-Test-Validation Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b42dd501-25ba-4b7c-88a0-15a45380428d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: 210 | Val: 45 | Test: 45\n"
     ]
    }
   ],
   "source": [
    "# -Train/Test/Validation Split (70/15/15) ---\n",
    "train_df, temp_df = train_test_split(df, test_size=0.30, random_state=42, stratify=df[\"llm_label\"])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=42, stratify=temp_df[\"llm_label\"])\n",
    "\n",
    "print(f\" Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecf4e92e-45e5-4033-b444-0099d09d4a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved train/val/test splits.\n"
     ]
    }
   ],
   "source": [
    "# Save splits \n",
    "train_df.to_csv(\"/workspaces/ERP_Newsletter/data_processed/train.csv\", index=False)\n",
    "val_df.to_csv(\"/workspaces/ERP_Newsletter/data_processed/val.csv\", index=False)\n",
    "test_df.to_csv(\"/workspaces/ERP_Newsletter/data_processed/test.csv\", index=False)\n",
    "\n",
    "print(\" Saved train/val/test splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc1775d-0c4e-4237-9436-c2c813be5f52",
   "metadata": {},
   "source": [
    "# VADER MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26241464-7fa3-4e23-85c9-a4556c33accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VADER Model \n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_sentiment(df, text_col=\"text_for_vader\"):\n",
    "    \"\"\"Apply VADER sentiment and return dataframe with scores and labels\"\"\"\n",
    "    def get_scores(text):\n",
    "        scores = analyzer.polarity_scores(str(text))\n",
    "        return pd.Series({\n",
    "            \"vader_neg\": scores[\"neg\"],\n",
    "            \"vader_neu\": scores[\"neu\"],\n",
    "            \"vader_pos\": scores[\"pos\"],\n",
    "            \"vader_compound\": scores[\"compound\"]\n",
    "        })\n",
    "    \n",
    "    vader_scores = df[text_col].apply(get_scores)\n",
    "    df = pd.concat([df.reset_index(drop=True), vader_scores], axis=1)\n",
    "\n",
    "    # Map compound scores → sentiment label\n",
    "    def map_label(c):\n",
    "        if c >= 0.05:\n",
    "            return \"positive\"\n",
    "        elif c <= -0.05:\n",
    "            return \"critical\"   # use 'critical' instead of 'negative'\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "    \n",
    "    df[\"vader_label\"] = df[\"vader_compound\"].apply(map_label)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1e6f768-e535-44f6-aa33-beb63ed2e793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved train split with VADER → /workspaces/ERP_Newsletter/data_processed/train_with_vader.csv\n",
      "💾 Saved val split with VADER → /workspaces/ERP_Newsletter/data_processed/val_with_vader.csv\n",
      "💾 Saved test split with VADER → /workspaces/ERP_Newsletter/data_processed/test_with_vader.csv\n"
     ]
    }
   ],
   "source": [
    "# Apply to each split \n",
    "for name, split in {\"train\": train_df, \"val\": val_df, \"test\": test_df}.items():\n",
    "    scored = get_vader_sentiment(split, text_col=\"text_for_vader\")\n",
    "    path = f\"/workspaces/ERP_Newsletter/data_processed/{name}_with_vader.csv\"\n",
    "    scored.to_csv(path, index=False)\n",
    "    print(f\"💾 Saved {name} split with VADER → {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edadc00-4ef2-4618-896d-12a970d43632",
   "metadata": {},
   "source": [
    "# Evaluate Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41fc5bd0-19fe-4c37-bace-436c37f32250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the test set (already scored by VADER)\n",
    "test = pd.read_csv(\"/workspaces/ERP_Newsletter/data_processed/test_with_vader.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c97d81f7-c998-4a95-ad9a-cbd967fc09cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your LLM labels as the comparison target\n",
    "y_true = test[\"llm_label\"].astype(str)\n",
    "y_pred = test[\"vader_label\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cc26345-1fe7-4b39-a7a7-6d1f4d813bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class order\n",
    "labels_order = [\"positive\", \"neutral\", \"critical\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8cfeca6-85af-431c-9103-e6b681ca952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Confusion Matrix (rows=True, cols=Pred) ===\n",
      "               Pred_positive  Pred_neutral  Pred_critical\n",
      "True_positive              1             5              0\n",
      "True_neutral               1            30              1\n",
      "True_critical              2             5              0\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "print(\"\\n=== Confusion Matrix (rows=True, cols=Pred) ===\")\n",
    "print(pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred, labels=labels_order),\n",
    "    index=[f\"True_{c}\" for c in labels_order],\n",
    "    columns=[f\"Pred_{c}\" for c in labels_order]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5607f49b-20a6-4081-957d-4c2dc698c287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive      0.045     0.167     0.071         6\n",
      "     neutral      0.600     0.938     0.732        32\n",
      "    critical      0.000     0.000     0.000         7\n",
      "\n",
      "   micro avg      0.373     0.689     0.484        45\n",
      "   macro avg      0.215     0.368     0.268        45\n",
      "weighted avg      0.433     0.689     0.530        45\n",
      "\n",
      "\n",
      "=== Summary Metrics ===\n",
      "Accuracy: 0.373\n",
      "Macro F1: 0.201\n",
      "Weighted F1: 0.287\n"
     ]
    }
   ],
   "source": [
    "# Classification metrics\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_true, y_pred, labels=labels_order, digits=3))\n",
    "\n",
    "print(\"\\n=== Summary Metrics ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred):.3f}\")\n",
    "print(f\"Macro F1: {f1_score(y_true, y_pred, average='macro'):.3f}\")\n",
    "print(f\"Weighted F1: {f1_score(y_true, y_pred, average='weighted'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2cc63-d39f-4a6c-b26e-4a1b4374150a",
   "metadata": {},
   "source": [
    "# Inspect Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "954e699c-3761-4b20-9986-facd490fa69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ 52 disagreements found\n",
      "                                      id  \\\n",
      "60                                   NaN   \n",
      "82                                   NaN   \n",
      "79                                   NaN   \n",
      "6   b7c41fb0-5a01-4b91-a750-fdbe01ddc64c   \n",
      "77                                   NaN   \n",
      "50                                   NaN   \n",
      "58                                   NaN   \n",
      "41  7f84506f-4ce9-4c47-8955-4adb3e464e97   \n",
      "42  2f4d0e26-5f5c-4883-bd7f-0951a5e9fe0d   \n",
      "81                                   NaN   \n",
      "\n",
      "                                                 text llm_label vader_label  \\\n",
      "60                                                NaN       NaN    positive   \n",
      "82                                                NaN       NaN    critical   \n",
      "79                                                NaN       NaN    critical   \n",
      "6   DfE NI - Launches New Integrated Education Str...  positive     neutral   \n",
      "77                                                NaN       NaN    positive   \n",
      "50                                                NaN       NaN     neutral   \n",
      "58                                                NaN       NaN    positive   \n",
      "41  The Conversation - England's maths teacher rec...  critical    positive   \n",
      "42  PoliticsHome - Primary School Teacher Applican...  critical     neutral   \n",
      "81                                                NaN       NaN     neutral   \n",
      "\n",
      "    vader_compound  \n",
      "60          0.0772  \n",
      "82         -0.5232  \n",
      "79         -0.8519  \n",
      "6              NaN  \n",
      "77          0.6808  \n",
      "50          0.0000  \n",
      "58          0.6652  \n",
      "41          0.7650  \n",
      "42             NaN  \n",
      "81          0.0000  \n"
     ]
    }
   ],
   "source": [
    "# Where VADER and LLM disagree\n",
    "errors = test[test[\"llm_label\"] != test[\"vader_label\"]][\n",
    "    [\"id\", \"text\", \"llm_label\", \"vader_label\", \"vader_compound\"]\n",
    "]\n",
    "print(f\"\\n❌ {len(errors)} disagreements found\")\n",
    "print(errors.sample(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236bec1-ac03-4961-86c7-562261d75cfd",
   "metadata": {},
   "source": [
    "# Quick Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fbf8639-02cb-4ff2-ab95-76edf249afd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Sentiment by Theme:\n",
      "vader_label                          critical  neutral  positive  Total  \\\n",
      "new_theme                                                                 \n",
      "political_context_and_organisations         1       16         0     17   \n",
      "teacher_rrd                                 0       12         3     15   \n",
      "digital_ed                                  0       12         1     13   \n",
      "\n",
      "vader_label                          % Positive  % Critical  % Neutral  \n",
      "new_theme                                                               \n",
      "political_context_and_organisations         0.0         5.9       94.1  \n",
      "teacher_rrd                                20.0         0.0       80.0  \n",
      "digital_ed                                  7.7         0.0       92.3  \n"
     ]
    }
   ],
   "source": [
    "summary_by_theme = test.groupby([\"new_theme\", \"vader_label\"]).size().unstack(fill_value=0)\n",
    "summary_by_theme[\"Total\"] = summary_by_theme.sum(axis=1)\n",
    "summary_by_theme[\"% Positive\"] = (summary_by_theme[\"positive\"] / summary_by_theme[\"Total\"] * 100).round(1)\n",
    "summary_by_theme[\"% Critical\"] = (summary_by_theme[\"critical\"] / summary_by_theme[\"Total\"] * 100).round(1)\n",
    "summary_by_theme[\"% Neutral\"] = (summary_by_theme[\"neutral\"] / summary_by_theme[\"Total\"] * 100).round(1)\n",
    "\n",
    "print(\"\\n📊 Sentiment by Theme:\")\n",
    "print(summary_by_theme.sort_values(\"Total\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6657364-c061-4c23-b00e-72daba460d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
