{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb902065-390d-4341-8645-a2492ba7db16",
   "metadata": {},
   "source": [
    "# Imports & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814ee64c-aaee-4918-bc85-8701f82f15a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e1fafa-b4ef-48cc-b364-909788e3a5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 300 rows\n",
      "Loaded 300 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['id', 'newsletter_number', 'issue_date', 'new_theme', 'text', 'domain',\n",
       "       'organisation', 'org_group', 'year_quarter'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "\n",
    "sample = pd.read_csv(\"/workspaces/ERP_Newsletter/data_processed/sample_300_full.csv\")\n",
    "labels = pd.read_csv(\"/workspaces/ERP_Newsletter/data_processed/sample_llm_prelabeled.csv\")\n",
    "\n",
    "print(f\"Loaded {len(labels)} rows\")\n",
    "print(f\"Loaded {len(sample)} rows\")\n",
    "\n",
    "sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1cdfb50-b604-4fdb-8cfb-44abfde904fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "labels = labels.rename(columns={\"doc_id\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcad6dc-c24a-46ba-885d-f65db8b7adc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged dataset shape: (300, 12)\n",
      "Columns: ['id', 'newsletter_number', 'issue_date', 'new_theme', 'text', 'domain', 'organisation', 'org_group', 'year_quarter', 'llm_label', 'llm_confidence', 'llm_rationale']\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(sample, labels, on=[\"id\", \"text\"], how=\"inner\")\n",
    "print(f\"✅ Merged dataset shape: {df.shape}\")\n",
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2c5978-046e-40f4-9404-2119698f1ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Columns retained: ['id', 'text', 'new_theme', 'organisation', 'org_group', 'year_quarter', 'llm_label', 'llm_confidence']\n"
     ]
    }
   ],
   "source": [
    "keep_cols = [\n",
    "    \"id\",\n",
    "    \"text\",\n",
    "    \"new_theme\",\n",
    "    \"organisation\",\n",
    "    \"org_group\",\n",
    "    \"year_quarter\",\n",
    "    \"llm_label\",\n",
    "    \"llm_confidence\"\n",
    "]\n",
    "\n",
    "df = df[keep_cols].copy()\n",
    "\n",
    "print(f\"✅ Columns retained: {keep_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45f8ce-b0d9-4cd2-adca-0af689926f62",
   "metadata": {},
   "source": [
    "# Light Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b401ef5-5d07-4c78-af9f-6127b3689c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_for_vader'] = df['text'].fillna('')  # Ensure no NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751c9f89-9043-4a43-ba11-9d1bf2eec986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove URLs \n",
    "df['text_for_vader'] = df['text_for_vader'].str.replace(\n",
    "    r'http\\S+|www\\S+', '', regex=True\n",
    ").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "272f8b47-f67f-40ba-b932-d247c3e7a5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    300.000000\n",
      "mean      42.133333\n",
      "std       21.375029\n",
      "min        6.000000\n",
      "25%       26.000000\n",
      "50%       37.000000\n",
      "75%       52.000000\n",
      "max      125.000000\n",
      "Name: text_for_vader, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check text lengths (VADER works better on sentences/paragraphs)\n",
    "print(df['text_for_vader'].str.split().str.len().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f5dd8-a54a-4648-9445-77c7e95bcdbe",
   "metadata": {},
   "source": [
    "# Train-Test-Validation Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b42dd501-25ba-4b7c-88a0-15a45380428d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: 210 | Val: 45 | Test: 45\n"
     ]
    }
   ],
   "source": [
    "# -Train/Test/Validation Split (70/15/15) ---\n",
    "train_df, temp_df = train_test_split(df, test_size=0.30, random_state=42, stratify=df[\"llm_label\"])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=42, stratify=temp_df[\"llm_label\"])\n",
    "\n",
    "print(f\" Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf4e92e-45e5-4033-b444-0099d09d4a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved train/val/test splits.\n"
     ]
    }
   ],
   "source": [
    "# Save splits \n",
    "train_df.to_csv(\"/workspaces/ERP_Newsletter/data_processed/train.csv\", index=False)\n",
    "val_df.to_csv(\"/workspaces/ERP_Newsletter/data_processed/val.csv\", index=False)\n",
    "test_df.to_csv(\"/workspaces/ERP_Newsletter/data_processed/test.csv\", index=False)\n",
    "\n",
    "print(\" Saved train/val/test splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc1775d-0c4e-4237-9436-c2c813be5f52",
   "metadata": {},
   "source": [
    "# VADER MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26241464-7fa3-4e23-85c9-a4556c33accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VADER Model \n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_sentiment(df, text_col=\"text_for_vader\"):\n",
    "    \"\"\"Apply VADER sentiment and return dataframe with scores and labels\"\"\"\n",
    "    def get_scores(text):\n",
    "        scores = analyzer.polarity_scores(str(text))\n",
    "        return pd.Series({\n",
    "            \"vader_neg\": scores[\"neg\"],\n",
    "            \"vader_neu\": scores[\"neu\"],\n",
    "            \"vader_pos\": scores[\"pos\"],\n",
    "            \"vader_compound\": scores[\"compound\"]\n",
    "        })\n",
    "    \n",
    "    vader_scores = df[text_col].apply(get_scores)\n",
    "    df = df.join(vader_scores)\n",
    "\n",
    "    df[\"vader_label\"] = df[\"vader_compound\"].apply(\n",
    "        lambda c: \"positive\" if c >= 0.05 else (\"critical\" if c <= -0.05 else \"neutral\")\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1e6f768-e535-44f6-aa33-beb63ed2e793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train split with VADER → /workspaces/ERP_Newsletter/data_processed/train_with_vader.csv\n",
      "Saved val split with VADER → /workspaces/ERP_Newsletter/data_processed/val_with_vader.csv\n",
      "Saved test split with VADER → /workspaces/ERP_Newsletter/data_processed/test_with_vader.csv\n"
     ]
    }
   ],
   "source": [
    "# Apply to each split \n",
    "for name, split in {\"train\": train_df, \"val\": val_df, \"test\": test_df}.items():\n",
    "    scored = get_vader_sentiment(split, text_col=\"text_for_vader\")\n",
    "    path = f\"/workspaces/ERP_Newsletter/data_processed/{name}_with_vader.csv\"\n",
    "    scored.to_csv(path, index=False)\n",
    "    print(f\"Saved {name} split with VADER → {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edadc00-4ef2-4618-896d-12a970d43632",
   "metadata": {},
   "source": [
    "# Evaluate Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41fc5bd0-19fe-4c37-bace-436c37f32250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the test set (already scored by VADER)\n",
    "test = pd.read_csv(\"/workspaces/ERP_Newsletter/data_processed/test_with_vader.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c97d81f7-c998-4a95-ad9a-cbd967fc09cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your LLM labels as the comparison target\n",
    "y_true = test[\"llm_label\"].astype(str)\n",
    "y_pred = test[\"vader_label\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cc26345-1fe7-4b39-a7a7-6d1f4d813bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class order\n",
    "labels_order = [\"positive\", \"neutral\", \"critical\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8cfeca6-85af-431c-9103-e6b681ca952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Confusion Matrix (rows=True, cols=Pred) ===\n",
      "               Pred_positive  Pred_neutral  Pred_critical\n",
      "True_positive              5             1              0\n",
      "True_neutral              15            10              7\n",
      "True_critical              2             1              4\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "print(\"\\n=== Confusion Matrix (rows=True, cols=Pred) ===\")\n",
    "print(pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred, labels=labels_order),\n",
    "    index=[f\"True_{c}\" for c in labels_order],\n",
    "    columns=[f\"Pred_{c}\" for c in labels_order]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5607f49b-20a6-4081-957d-4c2dc698c287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive      0.227     0.833     0.357         6\n",
      "     neutral      0.833     0.312     0.455        32\n",
      "    critical      0.364     0.571     0.444         7\n",
      "\n",
      "    accuracy                          0.422        45\n",
      "   macro avg      0.475     0.572     0.419        45\n",
      "weighted avg      0.679     0.422     0.440        45\n",
      "\n",
      "\n",
      "=== Summary Metrics ===\n",
      "Accuracy: 0.422\n",
      "Macro F1: 0.419\n",
      "Weighted F1: 0.440\n"
     ]
    }
   ],
   "source": [
    "# Classification metrics\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_true, y_pred, labels=labels_order, digits=3))\n",
    "\n",
    "print(\"\\n=== Summary Metrics ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred):.3f}\")\n",
    "print(f\"Macro F1: {f1_score(y_true, y_pred, average='macro'):.3f}\")\n",
    "print(f\"Weighted F1: {f1_score(y_true, y_pred, average='weighted'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2cc63-d39f-4a6c-b26e-4a1b4374150a",
   "metadata": {},
   "source": [
    "# Inspect Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "954e699c-3761-4b20-9986-facd490fa69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ 26 disagreements found\n",
      "                                      id  \\\n",
      "18  17f6595b-5405-4414-8e01-897d5686bc96   \n",
      "23  9407f618-d7fc-4588-bf28-2bb6ba2a3ecc   \n",
      "22  d733e51e-8a74-4fb4-8392-a511c882d9d7   \n",
      "28  1d99f921-ac60-46bf-9d84-9de09359ddf1   \n",
      "35  c22b3758-1b67-4a5b-a74d-2db1a67061f7   \n",
      "16  2632b516-8632-4e33-ab08-d729028964e1   \n",
      "4   136858de-8e48-4aac-95cf-a8fb8c005a3b   \n",
      "6   b7c41fb0-5a01-4b91-a750-fdbe01ddc64c   \n",
      "34  7298e4d0-00d3-46b6-bdc8-ac551434a3d8   \n",
      "37  914f2011-717a-400c-9e22-a4463f2b3f07   \n",
      "\n",
      "                                                 text llm_label vader_label  \\\n",
      "18  Consultation: Curriculum for Wales: continuing...   neutral    positive   \n",
      "23  IFS - Support for children with disabilities a...   neutral    positive   \n",
      "22  CAPE - Building a National Agenda for Regional...   neutral    positive   \n",
      "28  SchoolsWeek - Schools wanted for AI lesson pla...   neutral    positive   \n",
      "35  The Conversation - Should you give your child ...   neutral    critical   \n",
      "16  BBC - Attainment gap widens in Scottish school...  critical    positive   \n",
      "4   The Guardian - Sweden to implement nationwide ...   neutral    critical   \n",
      "6   DfE NI - Launches New Integrated Education Str...  positive     neutral   \n",
      "34  Teaching Commission report - Shaping the futur...   neutral    positive   \n",
      "37  DfE - Generative AI in education: educator and...   neutral    positive   \n",
      "\n",
      "    vader_compound  \n",
      "18          0.0772  \n",
      "23          0.9153  \n",
      "22          0.8910  \n",
      "28          0.4939  \n",
      "35         -0.7269  \n",
      "16          0.6652  \n",
      "4          -0.5574  \n",
      "6           0.0000  \n",
      "34          0.0772  \n",
      "37          0.0516  \n"
     ]
    }
   ],
   "source": [
    "# Where VADER and LLM disagree\n",
    "errors = test[test[\"llm_label\"] != test[\"vader_label\"]][\n",
    "    [\"id\", \"text\", \"llm_label\", \"vader_label\", \"vader_compound\"]\n",
    "]\n",
    "print(f\"\\n❌ {len(errors)} disagreements found\")\n",
    "print(errors.sample(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236bec1-ac03-4961-86c7-562261d75cfd",
   "metadata": {},
   "source": [
    "# Quick Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fbf8639-02cb-4ff2-ab95-76edf249afd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Sentiment by Theme:\n",
      "vader_label                          critical  neutral  positive  Total  \\\n",
      "new_theme                                                                 \n",
      "political_context_and_organisations         3        6         8     17   \n",
      "teacher_rrd                                 6        4         5     15   \n",
      "digital_ed                                  2        2         9     13   \n",
      "\n",
      "vader_label                          % Positive  % Critical  % Neutral  \n",
      "new_theme                                                               \n",
      "political_context_and_organisations        47.1        17.6       35.3  \n",
      "teacher_rrd                                33.3        40.0       26.7  \n",
      "digital_ed                                 69.2        15.4       15.4  \n"
     ]
    }
   ],
   "source": [
    "summary_by_theme = test.groupby([\"new_theme\", \"vader_label\"]).size().unstack(fill_value=0)\n",
    "summary_by_theme[\"Total\"] = summary_by_theme.sum(axis=1)\n",
    "summary_by_theme[\"% Positive\"] = (summary_by_theme[\"positive\"] / summary_by_theme[\"Total\"] * 100).round(1)\n",
    "summary_by_theme[\"% Critical\"] = (summary_by_theme[\"critical\"] / summary_by_theme[\"Total\"] * 100).round(1)\n",
    "summary_by_theme[\"% Neutral\"] = (summary_by_theme[\"neutral\"] / summary_by_theme[\"Total\"] * 100).round(1)\n",
    "\n",
    "print(\"\\n📊 Sentiment by Theme:\")\n",
    "print(summary_by_theme.sort_values(\"Total\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e612b6-2a95-4597-91f1-db7a8c3daf76",
   "metadata": {},
   "source": [
    "# Compare VADER with manual labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2149c9e5-9141-4322-9d8e-858cfc025fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths ---\n",
    "BASE = \"/workspaces/ERP_Newsletter/data_processed\"\n",
    "p_train = f\"{BASE}/train_with_vader.csv\"\n",
    "p_val   = f\"{BASE}/val_with_vader.csv\"\n",
    "p_test  = f\"{BASE}/test_with_vader.csv\"\n",
    "p_manual = f\"{BASE}/sample_llm_vs_manual_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5e0aa5e-5049-4f5b-a7c8-65f2b88b18f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and combine VADER splits ---\n",
    "v_train = pd.read_csv(p_train)\n",
    "v_val   = pd.read_csv(p_val)\n",
    "v_test  = pd.read_csv(p_test)\n",
    "\n",
    "vader_all = pd.concat([v_train, v_val, v_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0afe96e8-976d-47ed-bf4f-f7cec7bc7804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep useful columns if present\n",
    "keep_vader_cols = [c for c in [\n",
    "    \"id\",\"text\",\"new_theme\",\"organisation\",\"org_group\",\"year_quarter\",\n",
    "    \"llm_label\",\"llm_confidence\",\"text_for_vader\",\n",
    "    \"vader_neg\",\"vader_neu\",\"vader_pos\",\"vader_compound\",\"vader_label\"\n",
    "] if c in vader_all.columns]\n",
    "\n",
    "vader_all = vader_all[keep_vader_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "104163dd-c268-4d58-bf25-b4d4a0835d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER combined: (300, 14)\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate on id (safest to keep the first occurrence)\n",
    "if \"id\" not in vader_all.columns:\n",
    "    raise ValueError(\"Column 'id' missing from VADER files. Ensure you saved it in the splits.\")\n",
    "vader_all = vader_all.drop_duplicates(subset=[\"id\"], keep=\"first\")\n",
    "\n",
    "print(f\"VADER combined: {vader_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b81c4175-0112-4550-979a-02e5a8fb31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load manual labels (explicit column name) ---\n",
    "manual = pd.read_csv(p_manual)\n",
    "needed = [\"doc_id\", \"manual_label\"]\n",
    "missing = [c for c in needed if c not in manual.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in manual file: {missing}\")\n",
    "\n",
    "manual = manual.drop_duplicates(subset=[\"doc_id\"], keep=\"first\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4d06413-484e-4ebd-a498-f9a0f6964b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>llm_label</th>\n",
       "      <th>manual_label</th>\n",
       "      <th>llm_confidence</th>\n",
       "      <th>llm_rationale</th>\n",
       "      <th>did</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, doc_id, text, llm_label, manual_label, llm_confidence, llm_rationale, did, id]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf5b3a97-bd74-4b15-9f3c-97b46f9f7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual[\"id\"] = manual[\"doc_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29f1175e-f1ee-4229-bf1b-e1475bd7b048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged (manual ∩ vader): (300, 22)\n"
     ]
    }
   ],
   "source": [
    "# --- Merge manual with VADER ---\n",
    "merged = pd.merge(manual, vader_all, on=\"id\", how=\"inner\", suffixes=(\"_manualsrc\",\"_vader\"))\n",
    "print(\"Merged (manual ∩ vader):\", merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2782b6f4-054e-47b5-aec8-938d922ff20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual-only IDs (not in VADER splits): 0\n"
     ]
    }
   ],
   "source": [
    "# Quick diagnostics: coverage\n",
    "ids_in_manual_not_vader = set(manual[\"id\"]) - set(vader_all[\"id\"])\n",
    "print(f\"Manual-only IDs (not in VADER splits): {len(ids_in_manual_not_vader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46117367-77b2-4f1c-a170-2cc3048e5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Normalize labels ---\n",
    "def norm_label(x):\n",
    "    if pd.isna(x): return x\n",
    "    s = str(x).strip().lower()\n",
    "    if s == \"negative\": s = \"critical\"   # align naming\n",
    "    return s\n",
    "\n",
    "merged[\"manual_label\"] = merged[\"manual_label\"].apply(norm_label)\n",
    "if \"vader_label\" in merged.columns:\n",
    "    merged[\"vader_label\"] = merged[\"vader_label\"].apply(norm_label)\n",
    "if \"llm_label\" in merged.columns:\n",
    "    merged[\"llm_label\"] = merged[\"llm_label\"].apply(norm_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1408ad14-638d-416d-a07f-2776d3b7f307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Confusion Matrix (Manual vs VADER) ===\n",
      "               Pred_positive  Pred_neutral  Pred_critical\n",
      "True_positive             37             4              3\n",
      "True_neutral              23             6              8\n",
      "True_critical             22            10             37\n",
      "\n",
      "=== Classification Report (Manual vs VADER) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive      0.451     0.841     0.587        44\n",
      "     neutral      0.300     0.162     0.211        37\n",
      "    critical      0.771     0.536     0.632        69\n",
      "\n",
      "    accuracy                          0.533       150\n",
      "   macro avg      0.507     0.513     0.477       150\n",
      "weighted avg      0.561     0.533     0.515       150\n",
      "\n",
      "Accuracy:  0.533\n",
      "Macro F1:  0.477\n",
      "Weighted F1: 0.515\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate VADER vs MANUAL ---\n",
    "labels_order = [\"positive\",\"neutral\",\"critical\"]\n",
    "eval_df = merged.dropna(subset=[\"manual_label\",\"vader_label\"]).copy()\n",
    "\n",
    "y_true = eval_df[\"manual_label\"].astype(str)\n",
    "y_pred_vader = eval_df[\"vader_label\"].astype(str)\n",
    "\n",
    "print(\"\\n=== Confusion Matrix (Manual vs VADER) ===\")\n",
    "print(pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred_vader, labels=labels_order),\n",
    "    index=[f\"True_{c}\" for c in labels_order],\n",
    "    columns=[f\"Pred_{c}\" for c in labels_order]\n",
    "))\n",
    "print(\"\\n=== Classification Report (Manual vs VADER) ===\")\n",
    "print(classification_report(y_true, y_pred_vader, labels=labels_order, digits=3))\n",
    "print(f\"Accuracy:  {accuracy_score(y_true, y_pred_vader):.3f}\")\n",
    "print(f\"Macro F1:  {f1_score(y_true, y_pred_vader, average='macro'):.3f}\")\n",
    "print(f\"Weighted F1: {f1_score(y_true, y_pred_vader, average='weighted'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b23c31-86b1-4ef9-9e15-e7526897bf26",
   "metadata": {},
   "source": [
    "# Diagnostics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9d3900b-ba03-41f2-95b5-a0b8b8760442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  True POSITIVE (n=6):\n",
      "    → VADER says Positive: 5 (83.3%)\n",
      "    → VADER says Neutral:  1 (16.7%)\n",
      "    → VADER says Critical: 0 (0.0%)\n",
      "\n",
      "  True NEUTRAL (n=32):\n",
      "    → VADER says Positive: 15 (46.9%)\n",
      "    → VADER says Neutral:  10 (31.2%)\n",
      "    → VADER says Critical: 7 (21.9%)\n",
      "\n",
      "  True CRITICAL (n=7):\n",
      "    → VADER says Positive: 2 (28.6%)\n",
      "    → VADER says Neutral:  1 (14.3%)\n",
      "    → VADER says Critical: 4 (57.1%)\n"
     ]
    }
   ],
   "source": [
    "#Threshold Analysis\n",
    "for true_label in ['positive', 'neutral', 'critical']:\n",
    "    subset = test[test['llm_label'] == true_label]\n",
    "    pos_count = (subset['vader_compound'] >= 0.05).sum()\n",
    "    neu_count = ((subset['vader_compound'] > -0.05) & (subset['vader_compound'] < 0.05)).sum()\n",
    "    crit_count = (subset['vader_compound'] <= -0.05).sum()\n",
    "    \n",
    "    print(f\"\\n  True {true_label.upper()} (n={len(subset)}):\")\n",
    "    print(f\"    → VADER says Positive: {pos_count} ({pos_count/len(subset)*100:.1f}%)\")\n",
    "    print(f\"    → VADER says Neutral:  {neu_count} ({neu_count/len(subset)*100:.1f}%)\")\n",
    "    print(f\"    → VADER says Critical: {crit_count} ({crit_count/len(subset)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e7c7ec1-10bb-4f4e-80f3-b94282712883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total errors: 26 / 45 (57.8%)\n",
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# Sample Misclassifications \n",
    "mistakes = test[test['llm_label'] != test['vader_label']].copy()\n",
    "print(f\"Total errors: {len(mistakes)} / {len(test)} ({len(mistakes)/len(test)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33b9ad9d-491d-4ebb-9162-d3a5d9e62c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common business terms in VADER lexicon:\n",
      "  ✗ implementation: NOT IN LEXICON\n",
      "  ✗ rollout: NOT IN LEXICON\n",
      "  ✗ migration: NOT IN LEXICON\n",
      "  ✗ transition: NOT IN LEXICON\n",
      "  ✗ upgrade: NOT IN LEXICON\n",
      "  ✗ budget: NOT IN LEXICON\n",
      "  ✗ cost: NOT IN LEXICON\n",
      "  ✗ savings: NOT IN LEXICON\n",
      "  ✓ efficiency: 1.50\n",
      "  ✗ productivity: NOT IN LEXICON\n",
      "  ✓ delay: -1.30\n",
      "  ✗ issue: NOT IN LEXICON\n",
      "  ✓ problem: -1.70\n",
      "  ✓ challenge: 0.30\n",
      "  ✓ risk: -1.10\n",
      "  ✓ success: 2.70\n",
      "  ✓ improvement: 2.00\n",
      "  ✓ benefit: 2.00\n",
      "  ✓ advantage: 1.00\n",
      "  ✓ effective: 2.10\n"
     ]
    }
   ],
   "source": [
    "# 4. Vocabulary Analysis\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Check if common business/ERP terms are in VADER lexicon\n",
    "business_terms = [\n",
    "    'implementation', 'rollout', 'migration', 'transition', 'upgrade',\n",
    "    'budget', 'cost', 'savings', 'efficiency', 'productivity',\n",
    "    'delay', 'issue', 'problem', 'challenge', 'risk',\n",
    "    'success', 'improvement', 'benefit', 'advantage', 'effective'\n",
    "]\n",
    "\n",
    "print(\"Common business terms in VADER lexicon:\")\n",
    "for term in business_terms:\n",
    "    if term in analyzer.lexicon:\n",
    "        print(f\"  ✓ {term}: {analyzer.lexicon[term]:.2f}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {term}: NOT IN LEXICON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f33013d8-435e-411b-98a0-1fad37143008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current thresholds: pos >= 0.05, crit <= -0.05 → Macro F1: 0.419\n",
      "Optimal thresholds: pos >= 0.25, crit <= -0.30 → Macro F1: 0.502\n"
     ]
    }
   ],
   "source": [
    "# Try different thresholds \n",
    "best_f1 = 0\n",
    "best_thresholds = None\n",
    "\n",
    "pos_thresholds = np.arange(0.0, 0.5, 0.05)\n",
    "neg_thresholds = np.arange(-0.5, 0.0, 0.05)\n",
    "\n",
    "for pos_thresh in pos_thresholds:\n",
    "    for neg_thresh in neg_thresholds:\n",
    "        if neg_thresh >= pos_thresh:\n",
    "            continue\n",
    "        \n",
    "        test['vader_label_new'] = test['vader_compound'].apply(\n",
    "            lambda c: \"positive\" if c >= pos_thresh else (\"critical\" if c <= neg_thresh else \"neutral\")\n",
    "        )\n",
    "        \n",
    "        f1 = f1_score(test['llm_label'], test['vader_label_new'], average='macro')\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thresholds = (pos_thresh, neg_thresh)\n",
    "\n",
    "print(f\"Current thresholds: pos >= 0.05, crit <= -0.05 → Macro F1: 0.419\")\n",
    "print(f\"Optimal thresholds: pos >= {best_thresholds[0]:.2f}, crit <= {best_thresholds[1]:.2f} → Macro F1: {best_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2bf44d-4472-48b3-a1bc-0ce7fe93ac56",
   "metadata": {},
   "source": [
    "# Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55302d85-74e8-434a-bd2d-f8d8eaba8999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set professional style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "COLORS = {'positive': '#2ecc71', 'neutral': '#95a5a6', 'critical': '#e74c3c'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e94b027-82d5-47eb-a35b-a094e53f92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "test = pd.read_csv(\"/workspaces/ERP_Newsletter/data_processed/test_with_vader.csv\")\n",
    "y_true = test[\"llm_label\"]\n",
    "y_pred = test[\"vader_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23136d2e-f104-4c37-bb28-f4afc8ea4b52",
   "metadata": {},
   "source": [
    "### Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2be18f9c-7fbf-4637-b199-762bdbae21e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 1_confusion_matrices.png\n"
     ]
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "labels_order = [\"positive\", \"neutral\", \"critical\"]\n",
    "\n",
    "# Default thresholds\n",
    "cm_default = confusion_matrix(y_true, y_pred, labels=labels_order)\n",
    "sns.heatmap(cm_default, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels_order, yticklabels=labels_order,\n",
    "            cbar_kws={'label': 'Count'}, ax=ax1)\n",
    "ax1.set_title('Default Thresholds\\n(pos ≥ 0.05, crit ≤ -0.05)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('True Label', fontsize=11)\n",
    "ax1.set_xlabel('Predicted Label', fontsize=11)\n",
    "\n",
    "# Optimized thresholds\n",
    "test['vader_optimized'] = test['vader_compound'].apply(\n",
    "    lambda c: \"positive\" if c >= 0.25 else (\"critical\" if c <= -0.30 else \"neutral\")\n",
    ")\n",
    "cm_optimized = confusion_matrix(y_true, test['vader_optimized'], labels=labels_order)\n",
    "sns.heatmap(cm_optimized, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=labels_order, yticklabels=labels_order,\n",
    "            cbar_kws={'label': 'Count'}, ax=ax2)\n",
    "ax2.set_title('Optimized Thresholds\\n(pos ≥ 0.25, crit ≤ -0.30)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('True Label', fontsize=11)\n",
    "ax2.set_xlabel('Predicted Label', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspaces/ERP_Newsletter/visualisations/1_confusion_matrices.png', \n",
    "            dpi=300, bbox_inches='tight')\n",
    "print(\"✅ Saved: 1_confusion_matrices.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e6ccd-fbf3-433a-a961-1ce52f5f36a3",
   "metadata": {},
   "source": [
    "# F1 Scores by Class (Before/After)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "56613399-7647-4f85-b70c-7436d44a6170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 2_f1_comparison.png\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Calculate F1 scores\n",
    "f1_default = [\n",
    "    f1_score(y_true == label, y_pred == label) \n",
    "    for label in labels_order\n",
    "]\n",
    "f1_optimized = [\n",
    "    f1_score(y_true == label, test['vader_optimized'] == label) \n",
    "    for label in labels_order\n",
    "]\n",
    "\n",
    "x = np.arange(len(labels_order))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, f1_default, width, label='Default', \n",
    "               color='#3498db', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, f1_optimized, width, label='Optimized',\n",
    "               color='#2ecc71', alpha=0.8)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Sentiment Class', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Performance by Class: Before vs After Optimization', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([l.capitalize() for l in labels_order])\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspaces/ERP_Newsletter/visualisations/2_f1_comparison.png',\n",
    "            dpi=300, bbox_inches='tight')\n",
    "print(\"✅ Saved: 2_f1_comparison.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f09662-60a0-4b89-99f9-67a03bb65c88",
   "metadata": {},
   "source": [
    "### VADER compound score distribution by true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "061086c4-0470-4a6d-9423-23711f7756d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 3_score_distributions.png\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, label in enumerate(labels_order):\n",
    "    subset = test[test['llm_label'] == label]['vader_compound']\n",
    "    \n",
    "    axes[idx].hist(subset, bins=15, color=COLORS[label], \n",
    "                   alpha=0.7, edgecolor='black', linewidth=1.2)\n",
    "    axes[idx].axvline(0.25, color='green', linestyle='--', linewidth=2, \n",
    "                      label='Positive threshold', alpha=0.8)\n",
    "    axes[idx].axvline(-0.30, color='red', linestyle='--', linewidth=2,\n",
    "                      label='Critical threshold', alpha=0.8)\n",
    "    axes[idx].axvline(0, color='gray', linestyle='-', linewidth=1,\n",
    "                      alpha=0.5)\n",
    "    \n",
    "    axes[idx].set_title(f'True Label: {label.upper()}\\n(n={len(subset)})',\n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('VADER Compound Score', fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].legend(fontsize=8, loc='upper right')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Distribution of VADER Scores by True Sentiment Label',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspaces/ERP_Newsletter/visualisations/3_score_distributions.png',\n",
    "            dpi=300, bbox_inches='tight')\n",
    "print(\"✅ Saved: 3_score_distributions.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a402419c-95fc-4eeb-8896-4ba59e2bdeac",
   "metadata": {},
   "source": [
    "### Overall metrics comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0cb5fdcb-fdd3-4615-b1c2-4862450de9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 4_metrics_summary.png\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.axis('off')\n",
    "\n",
    "# Calculate metrics\n",
    "def get_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return [acc, macro_f1, weighted_f1]\n",
    "\n",
    "default_metrics = get_metrics(y_true, y_pred)\n",
    "optimized_metrics = get_metrics(y_true, test['vader_optimized'])\n",
    "improvement = [(opt - def_) / def_ * 100 for opt, def_ in zip(optimized_metrics, default_metrics)]\n",
    "\n",
    "# Create table\n",
    "table_data = [\n",
    "    ['Metric', 'Default', 'Optimized', 'Improvement'],\n",
    "    ['Accuracy', f'{default_metrics[0]:.3f}', f'{optimized_metrics[0]:.3f}', \n",
    "     f'+{improvement[0]:.1f}%'],\n",
    "    ['Macro F1', f'{default_metrics[1]:.3f}', f'{optimized_metrics[1]:.3f}',\n",
    "     f'+{improvement[1]:.1f}%'],\n",
    "    ['Weighted F1', f'{default_metrics[2]:.3f}', f'{optimized_metrics[2]:.3f}',\n",
    "     f'+{improvement[2]:.1f}%']\n",
    "]\n",
    "\n",
    "table = ax.table(cellText=table_data, cellLoc='center', loc='center',\n",
    "                 colWidths=[0.25, 0.25, 0.25, 0.25],\n",
    "                 bbox=[0, 0, 1, 1])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Style header row\n",
    "for i in range(4):\n",
    "    table[(0, i)].set_facecolor('#34495e')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Style data rows\n",
    "colors = ['#ecf0f1', '#ffffff']\n",
    "for i in range(1, 4):\n",
    "    for j in range(4):\n",
    "        table[(i, j)].set_facecolor(colors[i % 2])\n",
    "        if j == 3:  # Improvement column in green\n",
    "            table[(i, j)].set_facecolor('#d5f4e6')\n",
    "            table[(i, j)].set_text_props(weight='bold', color='#27ae60')\n",
    "\n",
    "ax.set_title('VADER Performance: Threshold Optimization Results',\n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.savefig('/workspaces/ERP_Newsletter/visualisations/4_metrics_summary.png',\n",
    "            dpi=300, bbox_inches='tight')\n",
    "print(\"✅ Saved: 4_metrics_summary.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab2b74-bf22-44f7-95bc-3f48c2db409a",
   "metadata": {},
   "source": [
    "### Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b0776ce-ee21-477d-a3ca-5963689d27f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 5_class_distribution.png\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Load all splits\n",
    "train = pd.read_csv(\"/workspaces/ERP_Newsletter/data_processed/train.csv\")\n",
    "val = pd.read_csv(\"/workspaces/ERP_Newsletter/data_processed/val.csv\")\n",
    "\n",
    "datasets = [\n",
    "    ('Training Set\\n(n=210)', train['llm_label']),\n",
    "    ('Validation Set\\n(n=45)', val['llm_label']),\n",
    "    ('Test Set\\n(n=45)', test['llm_label'])\n",
    "]\n",
    "\n",
    "for idx, (title, data) in enumerate(datasets):\n",
    "    counts = data.value_counts()\n",
    "    colors_list = [COLORS[label] for label in counts.index]\n",
    "    \n",
    "    wedges, texts, autotexts = axes[idx].pie(\n",
    "        counts.values,\n",
    "        labels=[l.capitalize() for l in counts.index],\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=colors_list,\n",
    "        textprops={'fontsize': 10, 'weight': 'bold'}\n",
    "    )\n",
    "    \n",
    "    # Make percentage text white for better visibility\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "    \n",
    "    axes[idx].set_title(title, fontsize=12, fontweight='bold', pad=10)\n",
    "\n",
    "plt.suptitle('Class Distribution Across Dataset Splits (Stratified)',\n",
    "             fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspaces/ERP_Newsletter/visualisations/5_class_distribution.png',\n",
    "            dpi=300, bbox_inches='tight')\n",
    "print(\"✅ Saved: 5_class_distribution.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e0204-60aa-41b4-b0fc-36de4e6df373",
   "metadata": {},
   "source": [
    "### Precision-Recall by Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "324823bf-8de5-4414-bd4e-5c8221b45102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: 6_precision_recall.png\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Get precision and recall for optimized model\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_true, test['vader_optimized'], labels=labels_order\n",
    ")\n",
    "\n",
    "x = np.arange(len(labels_order))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, precision, width, label='Precision',\n",
    "               color='#3498db', alpha=0.8)\n",
    "bars2 = ax.bar(x, recall, width, label='Recall',\n",
    "               color='#e67e22', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, f1, width, label='F1 Score',\n",
    "               color='#2ecc71', alpha=0.8)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Sentiment Class', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Detailed Performance Metrics by Class (Optimized Model)',\n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([l.capitalize() for l in labels_order])\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspaces/ERP_Newsletter/visualisations/6_precision_recall.png',\n",
    "            dpi=300, bbox_inches='tight')\n",
    "print(\"✅ Saved: 6_precision_recall.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0540aa4-24eb-4135-84be-a5379d8dbccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
